from urllib.parse import urljoin
from email.utils import parsedate_to_datetime
from datetime import datetime
from calibre.web.feeds.news import BasicNewsRecipe


class MinhaRevista(BasicNewsRecipe):
    title = 'Minha Revista Diaria'
    description = 'Revista de Noticias e Novidades.'
    language = 'pt-BR'
    encoding = 'utf-8'

    max_articles_per_feed = 5
    ignore_duplicate_articles = {'url', 'title'}
    recursions = 1
    no_stylesheets = True

    preprocess_regexps = [
        (r'(<p[^>]*>)', r'\1<!--SPLIT-->')
    ]

    keep_only_tags = [
        dict(name='article'),
        dict(id='main-content'),
        dict(class_='content'),
    ]

    remove_tags = [
        dict(name='header'), dict(name='footer'),
        dict(class_='nav'), dict(class_='menu'),
        dict(class_='sidebar'), dict(id='comments'),
        dict(class_='ad'), dict(name='script'),
        dict(name='noscript'), dict(name='style'),
    ]

    feed_urls = {
        'JW - Novidades PT': 'https://www.jw.org/pt/Novidades/rss/WhatsNewWebArticles/feed.xml',
        'JW - Whats New EN': 'https://www.jw.org/en/whats-new/rss/WhatsNewWebArticles/feed.xml',
        'THE NEWS (Beehiiv)': 'https://rss.beehiiv.com/feeds/j9teVW9Qmi.xml',
    }

    def parse_index(self):
        ans = []
        for topic, url in self.feed_urls.items():
            articles = []
            soup = self.index_to_soup(url)

            # Suporta RSS (<item>) e Atom (<entry>)
            items = soup.find_all(['item', 'entry'])
            normalized = []

            for it in items:
                # Link (RSS: <link>texto; Atom: <link href="...">)
                link_tag = it.find('link')
                link = None
                if link_tag:
                    href = link_tag.get('href')
                    link = href.strip() if href else (link_tag.get_text(strip=True) or None)
                if not link:
                    continue

                # Título
                title_tag = it.find('title')
                title = title_tag.get_text(strip=True) if title_tag else link

                # Data (RSS: <pubDate>; Atom: <updated>/<published>)
                pub_date = None
                pub_tag = it.find('pubDate') or it.find('updated') or it.find('published')
                if pub_tag and pub_tag.get_text(strip=True):
                    text = pub_tag.get_text(strip=True)
                    pub_date = self.parse_date_safe(text)

                normalized.append({
                    'url': link,
                    'title': title,
                    'date': pub_date or datetime.min  # fallback para ordenação
                })

            # Ordena por data desc e pega os 5 mais recentes
            normalized.sort(key=lambda x: x['date'], reverse=True)
            for obj in normalized[:self.max_articles_per_feed]:
                articles.append(dict(url=obj['url'], title=obj['title']))

            if articles:
                ans.append((topic, articles))
        return ans

    def parse_date_safe(self, text):
        # Tenta RFC2822 (RSS: "Wed, 25 Oct 2025 12:34:56 GMT")
        try:
            return parsedate_to_datetime(text)
        except Exception:
            pass
        # Tenta ISO 8601 (Atom: "2025-10-25T12:34:56Z")
        for fmt in ('%Y-%m-%dT%H:%M:%SZ', '%Y-%m-%dT%H:%M:%S%z', '%Y-%m-%d'):
            try:
                return datetime.strptime(text, fmt)
            except Exception:
                continue
        return None

    def print_version(self, url):
        if 'beehiiv.com' in url:
            return None
        if 'jw.org' in url:
            return url
        return url
