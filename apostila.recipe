# -*- coding: utf-8 -*-
from urllib.parse import urljoin
from calibre.web.feeds.news import BasicNewsRecipe

class ApostilaMWB(BasicNewsRecipe):
    """
    Receita para baixar a "Apostila de Reunião Vida e Ministério" (MWB) completa
    do jw.org, incluindo as referencias de Nivel 3 (textos biblicos, brochuras).
    """

    # --- METADADOS DO E-BOOK ---
    title          = 'Apostila da Reunião (MWB)'
    language       = 'pt-BR'
    encoding       = 'utf-8'
    description    = 'Apostila de Reunião Vida e Ministério Cristão completa, extraida do jw.org.'

    # --- CONFIGURACAO DE REDE ---
    # Aumenta o tempo limite de 120s (padrao) para 600s (10 minutos)
    # [cite: timeout = 120.0]
    timeout = 600
    
    # Aumenta o atraso para 2 segundos entre os downloads para
    # evitar ser bloqueado pelo servidor.
    # [cite: delay = 0]
    delay = 2

    # Desabilita a compressao gzip, que pode ser usada para detectar bots
    # [cite: handle_gzip = True]
    handle_gzip = False

    # --- CONFIGURACAO DE EXTRACAO (Onde a magica acontece) ---

    # 1. NIVEL 1: O INDICE PRINCIPAL
    #    Este e o URL da pagina que lista todos os "capitulos".
    index_url = 'https://www.jw.org/pt/biblioteca/jw-apostila-do-mes/novembro-dezembro-2025-mwb/'

    # 2. **** MUDANÇA IMPORTANTE ****
    #    Definimos recursions = 0 para baixar APENAS as paginas
    #    listadas no indice (Nivel 2), mas nao os links dentro delas (Nivel 3).
    #    Isso evita o estouro de memoria (OOM Kill) no GitHub Actions.
    #    [cite: recursions = 0]
    recursions = 0 

    # 3. A REGRA DE OURO DA LIMPEZA
    #    Em *todas* as paginas baixadas (Indice, Nivel 2, e Nivel 3),
    #    remove tudo exceto o conteudo dentro da tag <article id="article">.
    #    [cite: keep_only_tags = []]
    keep_only_tags = [
        dict(id='article')
    ]

    # --- CONFIGURACOES ADICIONAIS DE LIMPEZA ---
    no_stylesheets = True
    remove_javascript = True

    def get_browser(self, *args, **kwargs):
        """
        Finge ser um navegador Chrome comum para evitar ser bloqueado
        pelo servidor (erro "Remote end closed connection").
        [cite: get_browser(*args, **kwargs)]
        """
        br = super().get_browser(*args, **kwargs)
        headers = [
            ('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36'),
            ('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'),
            ('Accept-Language', 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7'),
            ('Connection', 'keep-alive'),
            ('Upgrade-Insecure-Requests', '1')
        ]
        # Substitui a lista de headers inteira pela nossa
        br.headers = headers
        return br
    
    # Nao usamos feeds RSS, entao 'feeds' e 'use_embedded_content' 
    # nao sao necessarios aqui.

    # --- **** METODOS DE NIVEL 3 REMOVIDOS **** ---
    # As funcoes 'preprocess_html' e 'is_link_wanted' foram removidas
    # pois 'recursions = 0' e elas nao sao mais necessarias.

    # --- METODO DE EXTRACAO (Nivel 1 -> Nivel 2) ---

    def parse_index(self):
        """
        Baixa o 'index_url' (Nível 1) e extrai os links
        para os "capitulos" (Nível 2).
        [cite: parse_index()]
        """
        
        self.log(f"Baixando indice de: {self.index_url}")
        
        try:
            soup = self.index_to_soup(self.index_url)
        except Exception as e:
            self.log.error(f"Falha ao baixar o indice principal: {e}")
            return []

        main_content = soup.find('article', attrs={'id': 'article'})

        if not main_content:
            self.log.error("Nao foi possivel encontrar <article id='article'> na pagina de indice.")
            return []

        articles = []
        links = main_content.find_all('a')
        self.log(f"Encontrados {len(links)} links no indice principal.")

        for link in links:
            title = self.tag_to_string(link) # [cite: classmethod tag_to_string(tag, ...)]
            href = link.get('href')

            if not href or not title or href.startswith('#') or not title.strip():
                continue
                
            url = urljoin(self.index_url, href)

            articles.append({
                'title': title.strip(),
                'url': url
            })

        self.log(f"Processados {len(articles)} artigos (capitulos) validos.")
        
        # [cite: parse_index()]
        return [('Apostila da Reunião', articles)]
